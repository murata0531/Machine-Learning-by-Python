{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明日\t名詞,副詞可能,*,*,*,*,明日,アシタ,アシタ\n",
      "晴れる\t動詞,自立,*,*,一段,基本形,晴れる,ハレル,ハレル\n",
      "か\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\n",
      "な\t助詞,終助詞,*,*,*,*,な,ナ,ナ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#単語分割\n",
    "\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger(\" /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "# 形態素解析 \n",
    "result = tagger.parse(\"明日晴れるかな。\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['明日', '晴れる']\n"
     ]
    }
   ],
   "source": [
    "#ストップワード\n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "tagger.parse(\"\") \n",
    "# 形態素解析結果をリストで取得\n",
    "node = tagger.parseToNode(\"明日晴れるかな。\")\n",
    "\n",
    "result = []\n",
    "while node is not None:\n",
    "    # 品詞情報取得\n",
    "    hinshi = node.feature.split(\",\")[0]\n",
    "    if  hinshi in [\"名詞\"]:\n",
    "        # 表層形の取得\n",
    "        result.append(node.surface)\n",
    "    elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "        # 形態素情報から原形情報を取得\n",
    "        result.append(node.feature.split(\",\")[6])\n",
    "    node = node.next\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル作成完了\n"
     ]
    }
   ],
   "source": [
    "#作品分類用のデータを[aozora.model]として作成\n",
    "\n",
    "import zipfile\n",
    "import os.path\n",
    "import urllib.request as req\n",
    "import MeCab\n",
    "from gensim import models\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#Mecabの初期化\n",
    "mecab = MeCab.Tagger()\n",
    "mecab.parse(\"\")\n",
    "\n",
    "#学習対象とする青空文庫の作品リスト\n",
    "list = [\n",
    "    {\"auther\":{\n",
    "        \"name\":\"宮澤 賢治\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000081/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"銀河鉄道の夜\",\"zipname\":\"43737_ruby_19028.zip\"},\n",
    "        {\"name\":\"注文の多い料理店\",\"zipname\":\"1927_ruby_17835.zip\"},\n",
    "        {\"name\":\"セロ弾きのゴーシュ\",\"zipname\":\"470_ruby_3987.zip\"},\n",
    "        {\"name\":\"やまなし\",\"zipname\":\"46605_ruby_29758.zip\"},\n",
    "        {\"name\":\"どんぐりと山猫\",\"zipname\":\"43752_ruby_17595.zip\"},\n",
    "    ]},\n",
    "    {\"auther\":{\n",
    "        \"name\":\"芥川 竜之介\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000879/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"羅生門\",\"zipname\":\"127_ruby_150.zip\"},\n",
    "        {\"name\":\"鼻\",\"zipname\":\"42_ruby_154.zip\"},\n",
    "        {\"name\":\"河童\",\"zipname\":\"69_ruby_1321.zip\"},\n",
    "        {\"name\":\"歯車\",\"zipname\":\"42377_ruby_34744.zip\"},\n",
    "        {\"name\":\"老年\",\"zipname\":\"131_ruby_241.zip\"},\n",
    "    ]},\n",
    "    {\"auther\":{\n",
    "        \"name\":\"太宰 治\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000035/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"斜陽\",\"zipname\":\"1565_ruby_8220.zip\"},\n",
    "        {\"name\":\"走れメロス\",\"zipname\":\"1567_ruby_4948.zip\"},\n",
    "        {\"name\":\"津軽\",\"zipname\":\"2282_ruby_1996.zip\"},\n",
    "        {\"name\":\"お伽草紙\",\"zipname\":\"307_ruby_3042.zip\"},\n",
    "        {\"name\":\"人間失格\",\"zipname\":\"301_ruby_5915.zip\"},\n",
    "    ]},\n",
    "    {\"auther\":{\n",
    "        \"name\":\"夏目 漱石\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000148/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"吾輩は猫である\",\"zipname\":\"789_ruby_5639.zip\"},\n",
    "        {\"name\":\"坊っちゃん\",\"zipname\":\"752_ruby_2438.zip\"},\n",
    "        {\"name\":\"草枕\",\"zipname\":\"776_ruby_6020.zip\"},\n",
    "        {\"name\":\"虞美人草\",\"zipname\":\"761_ruby_1861.zip\"},\n",
    "        {\"name\":\"三四郎\",\"zipname\":\"794_ruby_4237.zip\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "#作品リストを取得してループ処理に渡す\n",
    "def book_list():\n",
    "    for novelist in list:\n",
    "        auther = novelist[\"auther\"]\n",
    "        for book in novelist[\"books\"]:\n",
    "            yield auther, book\n",
    "        \n",
    "#Zipファイルを開き、中の文書を取得する\n",
    "def read_book(auther, book):\n",
    "    zipname = book[\"zipname\"]\n",
    "    #Zipファイルが無ければ取得する\n",
    "    if not os.path.exists(zipname):\n",
    "        req.urlretrieve(auther[\"url\"] + zipname, zipname)\n",
    "    zipname = book[\"zipname\"]\n",
    "    #Zipファイルを開く\n",
    "    with zipfile.ZipFile(zipname,\"r\") as zf:\n",
    "        #Zipファイルに含まれるファイルを開く。今回のZIPは一つのテキストファイルのみ含\n",
    "        for filename in zf.namelist():\n",
    "            with zf.open(filename,\"r\") as f:\n",
    "                #今回読むファイルはShift-JISなので指定してデコードする\n",
    "                return f.read().decode(\"shift-jis\")\n",
    "\n",
    "#引数のテキストを分かち書きして配列にする \n",
    "def split_words(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words\n",
    "\n",
    "#作品リストをDoc2Vecが読めるTaggedDocument形式にし、配列に追加する\n",
    "documents = []\n",
    "#作品リストをループで回す\n",
    "for auther, book in book_list():\n",
    "    #作品の文字列を取得\n",
    "    words = read_book(auther, book)\n",
    "    #作品の文字列を分かち書きに\n",
    "    wakati_words = split_words(words)\n",
    "    #TaggedDocumentの作成　文書=分かち書きにした作品　タグ=作者:作品名\n",
    "    document = TaggedDocument(\n",
    "        wakati_words, [auther[\"name\"] + \":\" + book[\"name\"]])\n",
    "    documents.append(document)\n",
    "    \n",
    "#TaggedDocumentの配列を使ってDoc2Vecの学習モデルを作成\n",
    "model = models.Doc2Vec(\n",
    "    documents, dm=1, vector_size=300, window=5, min_count=1)\n",
    "\n",
    "#Doc2Vecの学習モデルを保存\n",
    "model.save('aozora.model')\n",
    "\n",
    "print(\"モデル作成完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 「宮沢 賢治:よだかの星」と似た作品は? ---\n",
      "[('宮澤 賢治:どんぐりと山猫', 0.9996234774589539), ('宮澤 賢治:やまなし', 0.9996147155761719), ('芥川 竜之介:老年', 0.9995146989822388)]\n",
      "\n",
      "--- 「芥川 龍之介:犬と笛」と似た作品は? ---\n",
      "[('芥川 竜之介:羅生門', 0.9995617866516113), ('芥川 竜之介:老年', 0.9993518590927124), ('夏目 漱石:吾輩は猫である', 0.9991893768310547)]\n",
      "\n",
      "--- 「太宰 治:純真」と似た作品は? ---\n",
      "[('芥川 竜之介:羅生門', 0.9994699954986572), ('芥川 竜之介:老年', 0.9991984367370605), ('宮澤 賢治:どんぐりと山猫', 0.998833417892456)]\n",
      "\n",
      "--- 「夏目 漱石:一夜」と似た作品は? ---\n",
      "[('夏目 漱石:虞美人草', 0.9990417957305908), ('夏目 漱石:草枕', 0.9968736171722412), ('太宰 治:お伽草紙', 0.9962616562843323)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#「aozora.model」の分類\n",
    "\n",
    "import urllib.request as req\n",
    "import zipfile\n",
    "import os.path \n",
    "import MeCab\n",
    "from gensim import models\n",
    "\n",
    "#Mecabの初期化\n",
    "mecab = MeCab.Tagger()\n",
    "mecab.parse(\"\")\n",
    "\n",
    "#保存したDoc2Vec学習モデルを読み込み \n",
    "model = models.Doc2Vec.load('aozora.model')\n",
    "\n",
    "#分類用のZipファイルを開き、中の文書を取得する \n",
    "def read_book(url, zipname):\n",
    "    if not os.path.exists(zipname):\n",
    "        req.urlretrieve(url, zipname)\n",
    "\n",
    "    with zipfile.ZipFile(zipname,\"r\") as zf:\n",
    "        for filename in zf.namelist():\n",
    "            with zf.open(filename,\"r\") as f:\n",
    "                return f.read().decode(\"shift-jis\")\n",
    "\n",
    "#引数のテキストを分かち書きして配列にする\n",
    "def split_words(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words\n",
    "\n",
    "#引数のタイトル、URLの作品を分類する \n",
    "def similar(title, url):\n",
    "    zipname = url.split(\"/\")[-1]\n",
    "        \n",
    "    words = read_book(url, zipname)\n",
    "    wakati_words = split_words(words)\n",
    "    vector = model.infer_vector(wakati_words)\n",
    "    print(\"--- 「\" + title + '」と似た作品は? ---')\n",
    "    print(model.docvecs.most_similar([vector],topn=3))\n",
    "    print(\"\")\n",
    "\n",
    "#各作家の作品を１つずつ分類 \n",
    "similar(\"宮沢 賢治:よだかの星\",\n",
    "        \"https://www.aozora.gr.jp/cards/000081/files/473_ruby_467.zip\")\n",
    "\n",
    "similar(\"芥川 龍之介:犬と笛\",\n",
    "        \"https://www.aozora.gr.jp/cards/000879/files/56_ruby_845.zip\")\n",
    "\n",
    "similar(\"太宰 治:純真\",\n",
    "        \"https://www.aozora.gr.jp/cards/000035/files/46599_ruby_24668.zip\")\n",
    "\n",
    "similar(\"夏目 漱石:一夜\",\n",
    "        \"https://www.aozora.gr.jp/cards/000148/files/1086_ruby_5742.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You -> おはよう\n",
      "Bot -> おはよう\n",
      "You -> 明日晴れるかな\n",
      "Bot -> 明日は雨ですか。\n",
      "You -> \n",
      "Bot -> さようなら\n"
     ]
    }
   ],
   "source": [
    "#マルコフ連鎖を使用した自動作文bot\n",
    "\n",
    "import MeCab\n",
    "import os,json,random\n",
    "\n",
    "dict_file = \"markov_dict.json\"\n",
    "dic = {}\n",
    "\n",
    "# 辞書への登録\n",
    "def regist_dic(wordlist):\n",
    "    global dic\n",
    "    w1 = \"\"\n",
    "    w2 = \"\"\n",
    "    \n",
    "    # 要素が3未満の場合は、何もしない\n",
    "    if len(wordlist) < 3 : return\n",
    "    \n",
    "    for w in wordlist :\n",
    "        word = w[0]\n",
    "        if word == \"\" or  word == \"\\r\\n\" or word == \"\\n\" : continue\n",
    "        # 辞書に単語を設定\n",
    "        if w1 and w2 :\n",
    "            set_dic(dic,w1, w2, word)\n",
    "        # 文末を表す語のの場合、連鎖をクリアする\n",
    "        if word == \"。\" or word == \"?\" or  word == \"？\" :\n",
    "            w1 = \"\"\n",
    "            w2 = \"\"\n",
    "            continue\n",
    "        # 次の前後関係を登録するために、単語をスライド\n",
    "        w1, w2 = w2, word\n",
    "    \n",
    "    # 辞書を保存\n",
    "    json.dump(dic, open(dict_file,\"w\", encoding=\"utf-8\"))\n",
    "\n",
    "# 辞書に単語を設定\n",
    "def set_dic(dic, w1, w2, w3):\n",
    "    # 新しい単語の場合は、新しい辞書オブジェクトを作成\n",
    "    if w1 not in dic : dic[w1] = {}\n",
    "    if w2 not in dic[w1] : dic[w1][w2] = {}\n",
    "    if w3 not in dic[w1][w2]: dic[w1][w2][w3] = 0\n",
    "    # 単語の出現数をインクリメントする\n",
    "    dic[w1][w2][w3] += 1\n",
    "\n",
    "# 応答文の作成 \n",
    "def make_response(word):\n",
    "    res = []\n",
    "    \n",
    "    # 「名詞」/「形容詞」/「動詞」は、文章の意図を示していることが多いと想定し、始点の単語とする。\n",
    "    w1 = word\n",
    "    res.append(w1)\n",
    "    w2 = word_choice(dic[w1])\n",
    "    res.append(w2)\n",
    "    while True:\n",
    "        # w1,w2の組み合わせから予想される、単語を選択\n",
    "        if w1 in dic and w2 in dic[w1] : w3 = word_choice(dic[w1][w2])\n",
    "        else : w3 = \"\"\n",
    "        res.append(w3)\n",
    "        # 文末を表す語の場合、作文を終了\n",
    "        if w3 == \"。\" or w3 == \"?\" or  w3 == \"？\"  or w3 == \"\" :  break\n",
    "        # 次の単語を選択するために、単語をスライド\n",
    "        w1, w2 = w2, w3\n",
    "    return \"\".join(res)\n",
    "        \n",
    "def word_choice(candidate):\n",
    "    keys = candidate.keys()\n",
    "    return random.choice(list(keys))\n",
    "\n",
    "# メイン処理 \n",
    "\n",
    "# 辞書がすでに存在する場合は、最初に読み込む\n",
    "if os.path.exists(dict_file):\n",
    "        dic = json.load(open(dict_file,\"r\"))\n",
    "        \n",
    "while True:\n",
    "    # 標準入力から入力を受け付け、「さようなら」が入力されるまで続ける\n",
    "    text = input(\"You -> \")\n",
    "    if text == \"\" or text == \"さようなら\" : \n",
    "        print(\"Bot -> さようなら\")\n",
    "        break\n",
    "\n",
    "    # 文章整形\n",
    "    if text[-1] != \"。\" and text[-1] != \"?\" and text[-1] != \"？\" : text +=\"。\"\n",
    "    \n",
    "    # 形態素解析\n",
    "    tagger = MeCab.Tagger(\" /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    tagger.parse(\"\") \n",
    "    node =  tagger.parseToNode(text)\n",
    "    \n",
    "    # 形態素解析の結果から、単語と品詞情報を抽出\n",
    "    wordlist = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi not  in [\"BOS/EOS\"]:\n",
    "            wordlist.append([node.surface,hinshi])\n",
    "        node = node.next\n",
    "    \n",
    "    # マルコフ連鎖の辞書に登録\n",
    "    regist_dic(wordlist)\n",
    "\n",
    "    # 応答文の作成\n",
    "    for w in wordlist:\n",
    "        word = w[0]\n",
    "        hinshi = w[1]\n",
    "        # 品詞が「感動詞」の場合は、単語をそのまま返す\n",
    "        if hinshi in [ \"感動詞\"] : \n",
    "            print(\"Bot -> \" + word)\n",
    "            break\n",
    "        # 品詞が「名詞」「形容詞」「動詞」の場合で、かつ、辞書に単語が存在する場合は、作文して返す\n",
    "        elif (hinshi in [ \"名詞\" ,\"形容詞\",\"動詞\"]) and (word in dic):\n",
    "            print(\"Bot -> \" + make_response(word))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 全てのテキストを巡回して単語データベースを作成する\n",
    "import os, glob\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 保存ファイル名\n",
    "savefile = \"./ok-spam.pickle\"\n",
    "# MeCabの準備 \n",
    "tagger = MeCab.Tagger()\n",
    "# 変数の準備 \n",
    "word_dic = {\"__id\": 0} # 単語辞書\n",
    "files = [] # 読み込んだ単語データを追加する\n",
    "\n",
    "# 指定したディレクトリ内のファイル一覧を読む \n",
    "def read_files(dir, label):\n",
    "    # テキストファイルの一覧を得る\n",
    "    files = glob.glob(dir + '/*.txt')\n",
    "    for f in files:\n",
    "        read_file(f, label)\n",
    "\n",
    "# ファイルを読む \n",
    "def read_file(filename, label):\n",
    "    words = []\n",
    "    # ファイルの内容を読む\n",
    "    with open(filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    files.append({\n",
    "        \"label\": label,\n",
    "        \"words\": text_to_ids(text)\n",
    "    })\n",
    "\n",
    "# テキストを単語IDのリストに変換\n",
    "def text_to_ids(text):\n",
    "    # 形態素解析 \n",
    "    word_s = tagger.parse(text)\n",
    "    words = []\n",
    "    # 単語を辞書に登録 \n",
    "    for line in word_s.split(\"\\n\"):\n",
    "        if line == 'EOS' or line == '': continue\n",
    "        word = line.split(\"\\t\")[0]\n",
    "        params = line.split(\"\\t\")[1].split(\",\")\n",
    "        hinsi = params[0] # 品詞\n",
    "        hinsi2 = params[1] # 品詞の説明\n",
    "        org = params[6] # 単語の原型\n",
    "        # 助詞・助動詞・記号・数字は捨てる \n",
    "        if not (hinsi in ['名詞', '動詞', '形容詞']): continue\n",
    "        if hinsi == '名詞' and hinsi2 == '数': continue\n",
    "        # 単語をidに変換 \n",
    "        id = word_to_id(org)\n",
    "        words.append(id)\n",
    "    return words\n",
    "\n",
    "# 単語をidに変換 \n",
    "def word_to_id(word):\n",
    "    # 単語が辞書に登録されているか？\n",
    "    if not (word in word_dic):\n",
    "        # 登録されていないので新たにIDを割り振る\n",
    "        id = word_dic[\"__id\"]\n",
    "        word_dic[\"__id\"] += 1\n",
    "        word_dic[word] = id\n",
    "    else:\n",
    "        # 既存の単語IDを返す\n",
    "        id = word_dic[word]\n",
    "    return id\n",
    "\n",
    "# 単語の頻出頻度のデータを作る \n",
    "def make_freq_data_allfiles():\n",
    "    y = []\n",
    "    x = []\n",
    "    for f in files:\n",
    "        y.append(f['label'])\n",
    "        x.append(make_freq_data(f['words']))\n",
    "    return y, x\n",
    "\n",
    "def make_freq_data(words):\n",
    "    # 単語の出現回数を調べる\n",
    "    cnt = 0\n",
    "    dat = np.zeros(word_dic[\"__id\"], 'float')\n",
    "    for w in words:\n",
    "        dat[w] += 1\n",
    "        cnt += 1\n",
    "    # 回数を出現頻度に直す\n",
    "    dat = dat / cnt\n",
    "    return dat\n",
    "\n",
    "# ファイルの一覧から学習用のデータベースを作る\n",
    "if __name__ == \"__main__\":\n",
    "    read_files(\"ok\", 0)\n",
    "    read_files(\"spam\", 1)\n",
    "    y, x = make_freq_data_allfiles()\n",
    "    # ファイルにデータを保存\n",
    "    pickle.dump([y, x, word_dic], open(savefile, 'wb'))\n",
    "    print(\"ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393939393939394\n",
      "0.9090909090909091\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "0.8787878787878788\n",
      "0.9090909090909091\n",
      "0.9696969696969697\n",
      "0.9090909090909091\n",
      "1.0\n",
      "0.9696969696969697\n",
      "0.9090909090909091\n",
      "0.9090909090909091\n",
      "1.0\n",
      "0.9393939393939394\n",
      "0.9393939393939394\n",
      "0.9090909090909091\n",
      "0.9696969696969697\n",
      "0.9393939393939394\n",
      "0.8787878787878788\n",
      "0.8181818181818182\n",
      "0.9090909090909091\n",
      "0.9696969696969697\n",
      "0.8787878787878788\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "0.8181818181818182\n",
      "1.0\n",
      "0.9393939393939394\n",
      "1.0\n",
      "0.9090909090909091\n",
      "0.8484848484848485\n",
      "0.9090909090909091\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "1.0\n",
      "0.8484848484848485\n",
      "1.0\n",
      "0.9393939393939394\n",
      "0.9090909090909091\n",
      "1.0\n",
      "0.9696969696969697\n",
      "0.8787878787878788\n",
      "0.9696969696969697\n",
      "1.0\n",
      "0.9696969696969697\n",
      "1.0\n",
      "0.9696969696969697\n",
      "0.9393939393939394\n",
      "0.8484848484848485\n",
      "0.9090909090909091\n",
      "0.9090909090909091\n",
      "0.9090909090909091\n",
      "0.8787878787878788\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "0.8787878787878788\n",
      "0.9090909090909091\n",
      "0.8787878787878788\n",
      "1.0\n",
      "0.9696969696969697\n",
      "0.9393939393939394\n",
      "0.8787878787878788\n",
      "0.9696969696969697\n",
      "0.9090909090909091\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "0.9393939393939394\n",
      "1.0\n",
      "0.9090909090909091\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "1.0\n",
      "0.9393939393939394\n",
      "0.9393939393939394\n",
      "0.9090909090909091\n",
      "1.0\n",
      "0.9393939393939394\n",
      "1.0\n",
      "0.9696969696969697\n",
      "0.9090909090909091\n",
      "0.8484848484848485\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "1.0\n",
      "0.9393939393939394\n",
      "0.9696969696969697\n",
      "0.9393939393939394\n",
      "0.9393939393939394\n",
      "0.9090909090909091\n",
      "0.9696969696969697\n",
      "0.9696969696969697\n",
      "1.0\n",
      "1.0\n",
      "----\n",
      "average= 0.9418181818181809\n"
     ]
    }
   ],
   "source": [
    "#学習とテストを繰り返して単語の頻出度を調べる\n",
    "\n",
    "import pickle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# データファイルの読込\n",
    "data_file = \"./ok-spam.pickle\"\n",
    "save_file = \"./ok-spam-model.pickle\"\n",
    "data = pickle.load(open(data_file, \"rb\"))\n",
    "y = data[0] # ラベル\n",
    "x = data[1] # 単語の出現頻度\n",
    "\n",
    "# 100回、学習とテストを繰り返す \n",
    "count = 100\n",
    "rate = 0\n",
    "for i in range(count):\n",
    "    # データを学習用とテスト用に分割 \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2)\n",
    "    # 学習する\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    # 評価する\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    # 評価結果が良ければモデルを保存 \n",
    "    if acc > 0.94: pickle.dump(model, open(save_file, \"wb\"))\n",
    "    print(acc)\n",
    "    rate += acc\n",
    "# 平均値を表示 \n",
    "print(\"----\")\n",
    "print(\"average=\", rate / count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 結果= OK\n",
      "- 結果= SPAM\n"
     ]
    }
   ],
   "source": [
    "#スパム判定\n",
    "\n",
    "import pickle\n",
    "import MeCab\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# テストするテキスト\n",
    "test_text1 = \"\"\"\n",
    "明日晴れるかな\n",
    "\"\"\"\n",
    "test_text2 = \"\"\"\n",
    "億万長者になる方法を教えます。\n",
    "すぐに以下のアドレスに返信して。\n",
    "\"\"\"\n",
    "# ファイル名\n",
    "data_file = \"./ok-spam.pickle\"\n",
    "model_file = \"./ok-spam-model.pickle\"\n",
    "label_names = ['OK', 'SPAM']\n",
    "# 単語辞書を読み出す \n",
    "data = pickle.load(open(data_file, \"rb\"))\n",
    "word_dic = data[2]\n",
    "# MeCabの準備\n",
    "tagger = MeCab.Tagger()\n",
    "# 学習済みモデルを読み出す \n",
    "model = pickle.load(open(model_file, \"rb\"))\n",
    "\n",
    "# テキストがスパムかどうか判定する \n",
    "def check_spam(text):\n",
    "    # テキストを単語IDのリストに変換し単語の頻出頻度を調べる\n",
    "    zw = np.zeros(word_dic['__id'])\n",
    "    count = 0\n",
    "    s = tagger.parse(text)\n",
    "    # 単語毎の回数を加算 \n",
    "    for line in s.split(\"\\n\"):\n",
    "        if line == \"EOS\": break\n",
    "        params = line.split(\"\\t\")[1].split(\",\")\n",
    "        org = params[6] # 単語の原型\n",
    "        if org in word_dic:\n",
    "            id = word_dic[org]\n",
    "            zw[id] += 1\n",
    "            count += 1\n",
    "    zw = zw / count\n",
    "    # 予測 \n",
    "    pre = model.predict([zw])[0]\n",
    "    print(\"- 結果=\", label_names[pre])\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    check_spam(test_text1)\n",
    "    check_spam(test_text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
